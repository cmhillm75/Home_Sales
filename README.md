# Home_Sales

This assignment is part of Module 22: Big Data. It utilizes the edX dataset available at "<https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv>".

Using the provided dataset, you will leverage Spark to create temporary views and partition the data based on the `date_built` column, which is an integer representing the year the home was built. Following this, you will answer a series of questions about average home prices through SQL queries, using the DataFrame schema outlined below.

## Example: Checking the Range of Years in the Dataset

To get started, here is a simple example of a query you might run to check the range of years present in the `home_sales` dataset:

```python
# Ensure the home_sales dataset is loaded into a Spark temporary table.

query = """
    SELECT
        MIN(YEAR(date)) AS min_year,
        MAX(YEAR(date)) AS max_year
    FROM home_sales
"""

# Run the Query
year_range = spark.sql(query)

# Display the results
year_range.show()
```

* Query Output

```PlainText
+--------+--------+
|min_year|max_year|
+--------+--------+
|    2019|    2022|
+--------+--------+
```

```PlainText
root
 |-- id: string (nullable = true)
 |-- date: date (nullable = true)
 |-- date_built: integer (nullable = true)
 |-- price: integer (nullable = true)
 |-- bedrooms: integer (nullable = true)
 |-- bathrooms: integer (nullable = true)
 |-- sqft_living: integer (nullable = true)
 |-- sqft_lot: integer (nullable = true)
 |-- floors: integer (nullable = true)
 |-- waterfront: integer (nullable = true)
 |-- view: integer (nullable = true)
```

## Repo Directory

```PlainText
deep-learning-challenge/
├── Dockerfile
├── Home_Sales.ipynb
├── LICENSE
├── README.md
├── requirements.txt
```

## Docker Setup - I used Docker Desktop to create a Linux-based setup to run PySpark. If your system runs PySpark without issue or you use Google Colab, skip the Docker Setup

* See the `Dockerfile` in the repo for setting up the required installs.
* Ensure the `requirements.txt` file is downloaded.
* Build the Docker image:

```bash
  docker build -t home-sales .
```

* Run the Docker Container with the necessary ports exposed for Jupyter Notebook (8888) and Spark UI (4041)

```bash
    docker run -p 8888:8888 -p 4041:4041 home-sales
```

* Once the container is running, you can access the url through Docker Desktop by clicking on the Name column. The log will be present in the page that follows and a hyperlink <http://127.0.0.1:8888/tree?token=f92c185f0aa5fab4ebe1645afdc0798a35642bcb05b43451> will be present to open Jupyter Notebook. From there upload the `Home_Sales.ipynb`.
* To stop the container, close down Jupyter Notebook and either click the stop button in Docker Desktop or run the command `docker stop home-sales` in the terminal.
* To remove the container, run `docker rm home-sales` in the terminal or select Delete in Docker Desktop.

## Jupyter Setup

* The Jupyter setup instructions are only necessary if you plan to run `Home_Sales.ipynb` interactively in a notebook environment.

```python
# Import findspark and initialize.
import findspark
findspark.init()
```

```python
# Import packages
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
import time
from pyspark import SparkFiles
```

## References

Data for this dataset was generated by edX Boot Camps LLC, and is intended for educational purposes only.

## License

This project is licensed under the terms of the GNU General Public License v3.0. For more details, see the [LICENSE](https://www.gnu.org/licenses/gpl-3.0.en.html) file.
